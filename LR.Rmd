---
title: "UK / South Korea Trade: A Bayesian Analysis"
author: "Dominic Steinitz"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

FIXME?
```{r}
set.seed(42)
```

FIXME
http://www.bbc.co.uk/news/live/business-38452154
http://www.huffingtonpost.co.uk/entry/top-economist-slams-entirely-fictional-brexit-report_uk_586b747ee4b0d590e44f482f?
https://www.theguardian.com/commentisfree/2017/jan/03/brexit-create-4000000-jobs-economist-change-britain?CMP=twt_gu

I was intrigued by a
[tweet](https://twitter.com/PHammondMP/status/809628170471116800) by
the UK Chancellor of the Exchequer stating "exports have doubled over
the last year. Now worth nearly £11bn” and a
[tweet](https://twitter.com/JamesBerryMP/status/809341634378866689) by
Member of the UK Parliament stating South Korea "our second fastest
growing trading partner". Although I have never paid much attention to
trade statistics, both these statements seemed surprising. But these
days it's easy enough to verify such statements. It's also an
opportunity to use the techniques I believe data scientists in
(computer) game companies use to determine how much impact a new
feature has on the game's consumers.

One has to be slightly careful with trade statistics as they can just
be goods or goods and services. It's amusing to note that when I
provide software and analyses to US organisations, I am included in
the services exports from the UK to the US. Let's analyse goods first
before moving on to goods and services.

First let's get hold of the quarterly data from the UK [Office of
National Statistics](https://www.ons.gov.uk).

```{r}
ukstats <- "https://www.ons.gov.uk"
bop <- "economy/nationalaccounts/balanceofpayments"
ds <- "datasets/tradeingoodsmretsallbopeu2013timeseriesspreadsheet/current/mret.csv"

mycsv <- read.csv(paste(ukstats,"file?uri=",bop,ds,sep="/"),stringsAsFactors=FALSE)
```

Now we can find the columns that refer to Korea.

```{r}
ns <- which(grepl("Korea", names(mycsv)))
length(ns)
names(mycsv[ns[1]])
names(mycsv[ns[2]])
names(mycsv[ns[3]])
```

Now we can pull out the relevant information and create a data frame
of it.

```{r}
korean <- mycsv[grepl("Korea", names(mycsv))]
imports <- korean[grepl("Imports", names(korean))]
exports <- korean[grepl("Exports", names(korean))]
balance <- korean[grepl("Balance", names(korean))]

df <- data.frame(mycsv[grepl("Title", names(mycsv))],
                 imports,
                 exports,
                 balance)
colnames(df) <- c("Title", "Imports", "Exports", "Balance")

startQ <- which(grepl("1998 Q1",df$Title))
endQ <- which(grepl("2016 Q3",df$Title))
dfQ <- df[startQ:endQ,]
```


We can now plot the data.

```{r}
library(zoo)
tab <- data.frame(kr=as.numeric(dfQ$Exports),
                  krLabs=as.numeric(as.Date(as.yearqtr(dfQ$Title,format='%Y Q%q'))))

library(ggplot2)
ggplot(tab, aes(x=as.Date(tab$krLabs), y=tab$kr)) + geom_line() +
    theme(legend.position="bottom") +
    ggtitle("UK Export to South Korea (Quarterly)") +
    theme(plot.title = element_text(hjust = 0.5)) +
    xlab("Date") +
    ylab("Value (£m)")
```

For good measure let's plot the annual data.

```{r}
startY <- grep("^1998$",df$Title)
endY <- grep("^2015$",df$Title)
dfYear <- df[startY:endY,]

tabY <- data.frame(kr=as.numeric(dfYear$Exports),
                   krLabs=as.numeric(dfYear$Title))

ggplot(tabY, aes(x=tabY$krLabs, y=tabY$kr)) + geom_line() +
    theme(legend.position="bottom") +
    ggtitle("UK Export to South Korea (Annual)") +
    theme(plot.title = element_text(hjust = 0.5)) +
    xlab("Date") +
    ylab("Value (£m)")
```

And the monthly data.

```{r chunkname, fig.align='center'}
startM <- grep("1998 JAN",df$Title)
endM <- grep("2016 OCT",df$Title)
dfMonth <- df[startM:endM,]

tabM <- data.frame(kr=as.numeric(dfMonth$Exports),
                   krLabs=as.numeric(as.Date(as.yearmon(dfMonth$Title,format='%Y %B'))))

ggplot(tabM, aes(x=as.Date(tabM$krLabs), y=tabM$kr)) + geom_line() +
    theme(legend.position="bottom") +
    ggtitle("UK Export to South Korea (Monthly)") +
    theme(plot.title = element_text(hjust = 0.5)) +
    xlab("Date") +
    ylab("Value (£m)")
```

It looks like some change took place in 2011 but nothing to suggest
either that "export have doubled over the last year" or that South
Korea is "our second fastest growing partner". That some sort of
change did happen is further supported by the fact a [Free Trade
Agreement](http://trade.ec.europa.eu/doclib/press/index.cfm?id=1521)
between the EU and Korea was put in place in 2011.

But was there really a change? And what sort of change was it?
Sometimes it's easy to imagine patterns where
there are none.

<center>![](http://imgs.xkcd.com/comics/linear_regression.png)</center>

With this warning in mind let us see if we can get a better feel from
the numbers as to what happened.


# The Model

Let us assume that the data for exports are approximated by a linear
function of time but that there is a change in the slope and the
offset at some point during observation.

$$
\begin{aligned}
\tau &\sim {\mathrm{Uniform}}(1, N) \\
\mu_1 &\sim \mathcal{N}(\mu_{\mu_1}, \sigma_{\mu_1}) \\
\gamma_1 &\sim \mathcal{N}(\mu_{\gamma_1}, \sigma_{\gamma_1}) \\
\sigma_1 &\sim \mathcal{N}(\mu_{\sigma_1}, \sigma_{\sigma_1}) \\
\mu_2 &\sim \mathcal{N}(\mu_{\mu_2}, \sigma_{\mu_2}) \\
\gamma_2 &\sim \mathcal{N}(\mu_{\gamma_2}, \sigma_{\gamma_2}) \\
\sigma_2 &\sim \mathcal{N}(\mu_{\sigma_2}, \sigma_{\sigma_2}) \\
y_i &\sim \begin{cases} \mathcal{N}(\mu_1 x_i + \gamma_1, \sigma_1) & \mbox{if } i  \lt \tau \\ \mathcal{N}(\mu_2 x_i + \gamma_2, \sigma_2), & \mbox{if } i  \geq \tau \end{cases}
\end{aligned}
$$

Since we are going to use [stan](http://mc-stan.org) to infer the
parameters for this model and stan cannot handle discrete parameters,
we need to marginalize out this (discrete) parameter. I hope to do the
same analysis with [LibBi](http://libbi.org) which seems more suited
to time series analysis and which I believe will not require such a
step.

Setting $D=\{y_i\}_{i = 1}^N$ we can calculate the likelihood

$$
\begin{aligned}
p(D \,|\, \mu_1, \gamma_1, \sigma_1, \mu_2, \gamma_2, \sigma_2)
&= \sum_{n=1}^N p(\tau, D \,|\, \mu_1, \gamma_1, \sigma_1, \mu_2, \gamma_2, \sigma_2) \\
&= \sum_{\tau=1}^N p(\tau) p(D \,|\, \tau, \mu_1, \sigma_1, \mu_2, \sigma_2) \\
&=\sum_{\tau=1}^N p(\tau) \prod_{i=1}^N p(y_i \,|\, \tau, \mu_1, \gamma_1, \sigma_1, \mu_2, \gamma_2, \sigma_2)
\end{aligned}
$$

stan operates on the log scale and thus requires the log likelihood

$$
\log p(D \,|\, \mu_1, \gamma_1, \sigma_1, \mu_2, \gamma_2, \sigma_2) =
\mathrm{log\_sum\_exp}_{\tau=1}^T
\big(
  \log \mathcal{U}(\tau \, | \, 1, T) \\
  + \sum_{i=1}^T \log \mathcal{N}(y_i \, | \, \nu_i, \rho_i)
  \big)
$$

where

$$
\begin{aligned}
  \nu_i &=
  \begin{cases}
    \mu_1 x_i + \gamma_1 & \mbox{if } i < \tau \\
    \mu_2 x_i + \gamma_2 & \mbox{if } i  \geq \tau
  \end{cases} \\
  \rho_i &=
  \begin{cases}
    \sigma_1 & \mbox{if } i < \tau \\
    \sigma_2 & \mbox{if } i  \geq \tau
  \end{cases}
\end{aligned}
$$

and where the log sum of exponents function is defined by

$$
\mathrm{\log\_sum\_exp}_{n=1}^N \, \alpha_n
=
\log \sum_{n=1}^N \exp(\alpha_n).
$$

The log sum of exponents function allows the model to be coded
directly in Stan using the built-in function \code{log\_sum\_exp},
which provides both arithmetic stability and efficiency for mixture
model calculations.

## Stan

[stan](http://mc-stan.org) is a C++ library that comes with a domain specific modelling language very similar to statistical notation. Stan can help us to perform the Bayesian analysis steps, including MCMC inference (and more advanced techniques). The model that I will illustrate in the remainder of this document, is an adaptation of the [Coal Mine Distater](https://pymc-devs.github.io/pymc/tutorial.html) example from [Stan's manual](https://github.com/stan-dev/stan/releases/download/v2.12.0/stan-reference-2.12.0.pdf) Section 12. The full code of this model can be found at [https://gist.github.com/gmodena/0f316232aa2e9f7b6fc76b49f14bfb31](https://gist.github.com/gmodena/0f316232aa2e9f7b6fc76b49f14bfb31).


Here's the model in stan.

~~~~{.CPP include="lr-changepoint-ng.stan"}
~~~~

```{r}
NM <- nrow(tabM)
KM <- ncol(tabM)

yM <- tabM$kr
XM <- data.frame(tabM,rep(1,NM))[,2:3]

fitM <- stan(
    file = "lr-changepoint-ng.stan",
    data = list(x = XM$krLabs, y = yM, N = length(yM)),
    chains = 4,
    warmup = 1000,
    iter = 10000,
    cores = 4,
    refresh = 500,
    seed=42
)

histData <- hist(extract(fitM)$tau,plot=FALSE,breaks=c(seq(1,length(yM),1)))
histData$counts
```

A stan program is structured in blocks of code. Each block defines: 
1. the data
2. the parameters of our model
3. the model
4. some transformations of the model output


### Data

In the `data` block we describe what the dataset looks like. We will be modelling a time series `D`, stored as an array of `N`, `real` valued, elements.
We require `D` to have at least one element (`int<lower=1> N`).

```{markdown}
data {
    int<lower=1> N;
    real D[N]; 
}
```


### Parameters

The `parameter` block describes the sampling space.

In our example, we want to module two Gaussian, each with a `mu` and `sigma`. We constrain `sigma1` and `sigma2` to be positive
(`{markdown}<lower=0>`), so that we can stick an half-normal prior on them later on.
```{markdown}
parameters {
    real mu1;
    real mu2;

    real<lower=0> sigma1;
    real<lower=0> sigma2;
}
```

Stan does not allow for sampling from discrete distributions, so we will have to reparametrize our model and marginalise out  $\tau$. Let's proceed one step at a time.

We know that $P(D|\tau,\mu_1,\sigma_1,\mu_2,\sigma_2)=\frac{P(D,\tau,\mu_1,\sigma_1,\mu_2,\sigma_2)}{P(\tau,\mu_1,\sigma_1,\mu_2,\sigma_2)}$. It follows that the joint probability distribution factors as $P(D,\tau,\mu_1,\sigma_1,\mu_2,\sigma_2)=P(D|\tau,\mu_1,\sigma_1,\mu_2,\sigma_2)P(\tau,\mu_1,\sigma_1,\mu_2,\sigma_2)$.

To marginalize out $\tau$ we'll consider a factorization into likelihood and prior as $P(D,\mu_1,\sigma_1,\mu_2,\sigma_2) = P(D|\mu_1,\sigma_1,\mu_2,\sigma_2)P(\mu_1,\sigma_1,\mu_2,\sigma_2)$.

Then we calculate the likelihood $P(D|\mu_1, \sigma_1, \mu_2, \sigma_2) = \sum_{n=1}^N P(\tau, D | \mu_1, \sigma_1, \mu_2, \sigma_2) = \sum_{\tau=1}^N P(\tau) P(D|\tau, \mu_1, \sigma_1, \mu_2, \sigma_2)=\sum_{\tau=1}^N P(\tau) \prod_{n=1}^N P(d_n|\tau, \mu_1, \sigma_1, \mu_2, \sigma_2)$. Where $P(D|\tau, \mu_1, \sigma_1, \mu_2, \sigma_2)$ is a product of Gaussians. 

The `transformed parameters` block is used to process the `parameters` before calculating the posterior. In the block that follows we marginalise out `tau` and calculate `log_p(D | mu1, sd1, mu2, sd2)`. Since `stan` works in logarithmic scale, we'll have to take the sum of the log PDFs (`normal_lpdf`).
```{markdown}
// TODO: we can calculate log_p(D | mu1, sd1, mu2, sd2) in 
// linear time with dynamic programming
transformed parameters {
      vector[N] log_p;
      real mu;
      real sigma;
      log_p = rep_vector(-log(N), N);
      for (tau in 1:N)
        for (n in 1:N) {
          mu = n < tau ? mu1 : mu2;
          sigma = i < tau ? sigma1 : sigma2;
          log_p[tau] = log_p[tau] + normal_lpdf(D[n] | mu, sigma);
      }
}
```

The functions `normal_lpdf` (and `normal_lpdf` used in the `model` block) allows us to write a model in log scale, as required by Stan.

### Model
 
In the `model` block we define the priors on $\mu_1$, $\mu_2$, $\sigma_1$, $\sigma_2$, and the log-likelihood $\sum_{n} = log_p(d_n | \mu_1, \sigma_1, \mu_2, \sigma_2)$. A reasonably good default choice is to use an *half-normal* prior on the scale parameters $\sigma_1, \sigma_2$(a negative scale would be ill defined!). Here I'm using large values for the scale parameter $\sigma$ to denote uncertantinty in the prior beliefs of the distribution.  See [Prior Choice Recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) for an overview and best practices. If we know more about the data, we could use more certain values, that is define more appropriate priors.


```{markdown}  
model {
    mu1 ~ normal(0, 100);
    mu2 ~ normal(0, 100);
    
    // scale parameters need to be > 0;
    // we constrained sigma1, sigma2 to be positive
    // so that stan interprets the following as half-normal priors
    sigma1 ~ normal(0, 100);
    sigma2 ~ normal(0, 100);
    
    target += log_sum_exp(log_p);
}
```

What about the posterior? This is where some of Stan's magic kicks in. At each iteration of MCMC sampling after convergence, $\mu_1, \sigma_1, \mu_2, \sigma_2$ are drawn from $P( \mu_1, \sigma_1, \mu_2, \sigma_2 | D)$, and $P(\tau| \mu_1, \sigma_1, \mu_2, \sigma_2, D)$ is calculated based on the local unnormalized value of `log_p`. As a final step $P(\tau | \mu_1, \sigma_1, \mu_2, \sigma_2, D)$ is normalized by averaging over all draws to obtain $P(\tau|D)$. More details on this step can be found in Stan's manual.

### Discrete sampling
The `generated quantities` block allows for postprocessing. We can use it to draw a discrete sampling of `tau` at each iteration using the `categorical_rng` probability distribution. At each iteration we draw a value of `tau`, and later on, we'll look at the histogram of draws to determine the most likely changepoint as the most frequent `tau`.
```{markdown}
generated quantities {
    int<lower=1,upper=N> tau;
    // K simplex are a data type in Stan
    simplex[N] sp;
    sp = softmax(log_p);
    tau = categorical_rng(sp);
}
```

The `softmax` transform maps `log_p` to a [K-simplex](https://en.wikipedia.org/wiki/Simplex#The_standard_simplex), which is the parameter type expected by `categorical_rng`. The label will be the index of `log_p`.


## Putting it all together

Let's generate some artificial data to test if the models works as expected.
```{r bayesian-changepoint-data}
x1 <- rnorm(41, mean=15, sd=1.5)
x2 <- rnorm(79, mean=17, sd=1.1)

x <- c(x1, x2)
plot(x, type='l')
```

If all goes as expected, a changepoint should be identified at $\tau = 42$.

### The R interface for Stan

I'll be using the excellent `rstan` package to fit the model and analyse its output.
```{r}
library(rstan)
rstan::stan_version()
rstan_options(auto_write = TRUE) # cache a compiled Stan program
```

The `stan` function wraps the following three steps:

1. Translate a model in Stan code to C++ code
2. Compile the C++ code to a dynamic shared object (DSO) and load the DSO
3. Sample given some user-specified data and other settings

The function returns an S4 `stanfit` object. We can use methods such as `print` and `plot` (and `pairs`) to check the fitted results.

```{r}
fit <- stan(
  file = "changepoint.stan",  
  data = list(D = x, N=length(x)),    
  chains = 4,             
  warmup = 1000,          
  iter = 10000,            
  cores = 4,              
  refresh = 500          
  )
```
The parameters are self explanatory. Note that the variables naming in the `data` parameter should match the `data` block in the stan code. 

`print` gives an overview of the parameters and the log posterior `log_p`. 
```{r}
print(fit)
```
Looking at the table, we can see that the `log_p` has a minimum at index 42. We can see this more explicitly if we look at a histogram of
the discrete values of `tau`

```{r bayesian-changepoint-tau-hist}
qplot(extract(fit)$tau, geom='histogram', xlab = "tau") 
print(fit, pars=c("tau"))
```

We also get the credible inteterval of $\tau$ using the `plot` function with
```{r bayesian-changepoint-ci}
plot(fit, pars=c("tau"))
```
The plot indicates the expected value of $\tau = 42$. The red bands denote the 80% credible interval; if we were to repeat the experiment several time, we'd expect $\tau$ to lie in the interval $41 \leq \tau \leq 44$ with probability $P=0.80$. The black bands denote the 95% credible interval.

This result is consistent with the dataset $D$.

## Conclusion
In this document I showed a simple yet powerful bayesian model for detecting a single changepoint in a timeseries. The advantages over the frequentist approach are twofold. On the one hand, the Bayesian model gives a distribution of changepoints and not a single point estimate. This lets us reason about how credible the model output is. On the other hand, we don't need to hard-code threshold values for a test statistic, but rather embed our prior knowledge about the data to "tune" the model.

It's not all roses, though. The flexibility given to us by the Bayesian framework comes at the expenses of some math being needed to work with a discrete parameter. This is largely due to `stan`s implementation choices wrt discrete random variables. For a more concise changepoint model, written with the PyMC framework one can have a look at the work of my colleague Vincent at [Switching to Sampling in order to Switch](http://koaning.io/switching-to-sampling-in-order-to-switch.html). A drawback of the Bayesian approach is that it is relatively computationally expensive. On my machine (a 2015 retina MacBook pro), it takes about 7 seconds to run the MCMC simulation on four cores[^2]. For comparison, the `changepoint` package has a sub-second run time.


[^1]: The excellent [changepoint](https://cran.r-project.org/web/packages/changepoint/index.html) R package implements this likelihood test, as well as its natural extension to the case of multiple change points.
[^2]: The `transformed parameters` block has a quadratic time complexity. Stan's manual contains a dynamic programming alternative with linear time complexity.
